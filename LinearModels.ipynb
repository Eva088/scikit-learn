{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Models**\n",
    "\n",
    "These are the methods for regression where the target variable is expected to be a linear combination of the features. \n",
    "\n",
    "**1.1.1 Ordinary Least Squares**\n",
    "\n",
    "$$ \\text{RSS} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "The goal of OLS is to estimate the coefficients that minimize the sum of squared residuals(errors) between the observed target values and the value of the target predicted by the linear approximation. \n",
    "\n",
    "LinearRegression will take in its fit method arrays X,y and will store the coefficients 'w' of the linear model in its coef_ member. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit([[0,0], [1,1], [2,2]], [0,1,2])\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLS relies on the assumption that features are independent. Multicollinearity occurs when features are highly correlated, which makes the design matrix nearly singular and the coefficient estimates unstable. When this happens, the model's predictions become very sensitive to small errors in the data, leading to high variance in the estimates. \n",
    "\n",
    "Example: If you're predicting house prices, and your features include both 'house size' and 'number of rooms', which are correlated, your model might gibr inconsistent or unreliable estimates of how each feature impacts price. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1.1.1. Non-negative Least Squares**\n",
    "\n",
    "This model constrains the coefficients to be non-negative by setting the parameter positive to be True. It is applicable while representing non-negative entities such as frequency counts or prices of goods. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.2 Ridge Regression and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.2.1 Regression\n",
    "\n",
    "Ridge regression introduces a regularization penalty that penalizes the size of all coefficients in the model, and in doing so, automatically reduces the impact of multicollinearity without explicitly detecting it. This penalty is proportional to the sum of squares of the coefficients. \n",
    "\n",
    "In other words, it shrinks the coefficients in proportion to their contribution to the model. The correlated predictors end up having their coefficients shrunk more because they provide redundant information. Predictors that are uncorrelated with others will receive less shrinkage because they are seen as contributing unique information to the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13636363636363638"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.Ridge(alpha = 0.5)\n",
    "reg.fit([[0,0], [0,0], [1,1]], [0, .1,1])\n",
    "reg.coef_\n",
    "reg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class Ridge allows for the user to specify that the solver be automatically chosen by setting solver = auto. When this option is specified, Ridge will choose between 'lbfgs', 'cholesky' and 'sparse_cg' solvers. Ridge will begin checking the conditions and choose the solver as follows: \n",
    "\n",
    "'lbfgs' - The positive = True option is specified. \n",
    "'cholesky' - The input array X is not sparse. \n",
    "'sparse_cg' - None of the above conditions are fulfilled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.2.2 Classification\n",
    "\n",
    "The ridge regressor has a classifier variant: RidgeClassifier. \n",
    "\n",
    "1. Binary Classification (eg Cat vs Dog)\n",
    "\n",
    "Imagine we have a dataset with 2 features (e.g size and weight) to classify images as either 'cat' or 'dog'\n",
    "\n",
    "**Step 1: Convert labels to {-1,1}**\n",
    "\n",
    "Normally, we might have labels like 0 for 'cat' and 1 for 'dog'.\n",
    "\n",
    "For RidgeClassifier, we convert these labels to {-1,1}\n",
    "\n",
    "**Step 2: Treat the problem as regression**\n",
    "\n",
    "-RidgeClassifier treats this as a regression problem and fits a ridge regression model to the data, trying to predict a continuous value. \n",
    "\n",
    "-For example, RidgeClassifier will try to find a model that best fits the data such that it predicts a value close to 1 for 'dog' and close to -1 for 'cat'. \n",
    "\n",
    "**Step 3: Predict and classify based on the sign**\n",
    "\n",
    "After fitting the model, RidgeClassifer predicts a continuous value (say 0.5 for a test example). If the prediction is positive (greater than or equal to 0), the model classifies it as +1 ('dog'). If the prediction is negative, the model classifies it as -1 ('cat'). \n",
    "\n",
    "For instance: \n",
    "\n",
    "-For an image of a dog, the model might predict a value like +0.8. Since it is positive, the model classifies it as 'dog'. \n",
    "\n",
    "-For an image of a cat, the model might predict -0.6. Since it is negative, the model classifies it as 'cat'. \n",
    "\n",
    "\n",
    "2. MultiClass Classification\n",
    "\n",
    "Let's consider a multiclass classification task where we classify data into 3 categories: 'cat', 'dog', 'rabbit'\n",
    "\n",
    "**Step 1: Setup multiclass classification**\n",
    "\n",
    "RidgeClassifier treats this as a multi-output regression probelm. \n",
    "\n",
    "For each class, it performs a separate regression and the model predicts a value for each class. \n",
    "\n",
    "**Step 2: Training the model**\n",
    "\n",
    "RidgeClassifier will fit the model and generate a set of coefficients for each class. \n",
    "\n",
    "For instance, it will predict a value for 'cat', a value for 'dog', and a value for 'rabbit' for each example in the training set. \n",
    "\n",
    "**Step 3: Predict the class with the highest value**\n",
    "\n",
    "After the model is trained, it will predict three values for a test instance, one for each class (cat, dog, rabbit). \n",
    "\n",
    "For example: \n",
    "\n",
    "Predicted values for a test instance might be: cat - 0.3, dog - 1.2, rabbit - (-0.5)\n",
    "\n",
    "RidgeClassifier will then classify the test instance as the class with the highest predicted value, which in this case is 'dog'. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
