{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction**\n",
    "\n",
    "Term Frequency - Inverse Document Frequency (Tf-idf) is an important tool in Natural Language Processing and is used by algorithms like cosine similarity to find documents that aare similar to a given search query. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Term Frequency\n",
    "\n",
    "Tf is the number of times a term appears in a particular document. It's specific to a document. \n",
    "\n",
    "tf(t) : The number of times 't' appears in a document\n",
    "\n",
    "2. Inverse Document Frequency \n",
    "\n",
    "idf is the measure of how common or rare a term is across the entire corpus of documents. It's common to all the documents. If a word is common to and appears in many documents, the idf value (normallized) will approach 0 or else approach 1 if it's rare. \n",
    "\n",
    "\n",
    "tf-idf value of a term in a document is the product of its tf and idf. The higher the value, the more relevant the term is in that document. \n",
    "\n",
    "Note 1 on Normalization: \n",
    "\n",
    "Tf-idf assigns a weight to each feature (term) based on Term Frequency (TF) and Inverse Document Frequency (IDF). The Term Frequency (TF) reflects how often the term appears in the document, and IDF reflects how rare or common the term is in the entire document.\n",
    "\n",
    "-The TF already ensures that more frequent terms in a document will have higher weightage. \n",
    "\n",
    "-The IDF ensures that terms that appear frequently across many documents (common words) will be given lower weightage.\n",
    "\n",
    "These two components ensure that important terms (those that are relevant to the document's context) are still emphasized by their TF-IDF scores, even after normalization. \n",
    "\n",
    "Normalization (making each document vector unit length) affects the magnitude of the vector but not the relative importance of individual features within that vector. The relative relationship between the tf-idf values of different features in a single document is preserved. \n",
    "\n",
    "Before normalization: Larger documents(with more terms) will have higher overall magnitudes (sum of sqaured tf-idf values)\n",
    "\n",
    "After normalization: THe magnitudes are adjusted to 1, but the relative importance of the features remains the same. So, the term 'cars' might have a higher weight compared to 'diesel' in a document, even after normalization. \n",
    "\n",
    "Note 2: \n",
    "\n",
    "When the IDF of a term is zero, it means that the term appears in every document in the corpus. This implies that the term is not informative or discriminative for distinguishing between different documents in the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['petrol cars are cheaper than diesel cars', 'diesel is cheaper than petrol']\n"
     ]
    }
   ],
   "source": [
    "#Importing the library\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Setting up the document corpus\n",
    "\n",
    "d1 = \"petrol cars are cheaper than diesel cars\"\n",
    "\n",
    "d2 = \"diesel is cheaper than petrol\"\n",
    "\n",
    "doc_corpus = [d1, d2]\n",
    "\n",
    "print(doc_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names n:  ['cars' 'cheaper' 'diesel' 'petrol']\n"
     ]
    }
   ],
   "source": [
    "#Initializing TfidfVectorizer and printing the feature names\n",
    "\n",
    "vec = TfidfVectorizer(norm = 'l2', stop_words = 'english') #the vectorizer will automatically exclude common english stop words\n",
    "\n",
    "#These words are typically not very informative for tasks like text classification or clustering\n",
    "matrix = vec.fit_transform(doc_corpus)\n",
    "\n",
    "print(\"Feature Names n: \", vec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse Matrix n (2, 4) n [[0.85135433 0.30287281 0.30287281 0.30287281]\n",
      " [0.         0.57735027 0.57735027 0.57735027]]\n"
     ]
    }
   ],
   "source": [
    "#Generate a sparse matrix with tf-idf values\n",
    "\n",
    "print('Sparse Matrix n', matrix.shape,'n', matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature names are: ['analysis' 'artificial' 'data' 'fields' 'great' 'growing' 'intelligence'\n",
      " 'involves' 'learning' 'love' 'machine' 'programming' 'python' 'science'\n",
      " 'statistics' 'subfield']\n",
      "Shape of Sparse Matrix: (5, 16) array: [[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.61418897 0.         0.61418897\n",
      "  0.49552379 0.         0.         0.        ]\n",
      " [0.55032913 0.         0.44400208 0.         0.55032913 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.44400208 0.         0.         0.        ]\n",
      " [0.         0.45109178 0.         0.         0.         0.\n",
      "  0.45109178 0.         0.37444693 0.         0.37444693 0.\n",
      "  0.         0.         0.         0.55911663]\n",
      " [0.         0.         0.37831623 0.         0.         0.\n",
      "  0.         0.46891321 0.31403664 0.         0.31403664 0.\n",
      "  0.         0.46891321 0.46891321 0.        ]\n",
      " [0.         0.39372848 0.         0.4880163  0.         0.4880163\n",
      "  0.39372848 0.         0.3268302  0.         0.3268302  0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#2.Example 2\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "d1 = \"I love programming in Python\"\n",
    "\n",
    "d2 = \"Python is great for data analysis\"\n",
    "\n",
    "d3 = \"Machine learning is a subfield of artificial intelligence\"\n",
    "\n",
    "d4 = \"Data Science involves machine learning and statistics\"\n",
    "\n",
    "d5 = \"Artificial intelligence and machine learning are growing fields\"\n",
    "\n",
    "documents = [d1,d2,d3,d4,d5]\n",
    "\n",
    "vec = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "matrix = vec.fit_transform(documents)\n",
    "\n",
    "print(f'The feature names are: {vec.get_feature_names_out()}')\n",
    "\n",
    "print('Shape of Sparse Matrix:', matrix.shape, 'array:', matrix.toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
