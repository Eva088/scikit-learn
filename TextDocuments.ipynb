{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification of text documents using sparse features**\n",
    "\n",
    "This example demonstrates the use of scikit-learn to classify documents by topics using a Bag Of Words approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading and vectorizing the 20 newsgroup dataset**\n",
    "\n",
    "We use The 20 newsgroups text dataset which comprises around 18,000 newsgroups posts on 20 topics split into two subsets: one for training (or development) and the other for testing (or performance evaluation). By default, the text samples contain some message metadata such as 'headers', 'footers' and 'quotes'. The feth_20newsgroups function accepts a parameter named 'remove' to strip such information that can make the classification problem 'too easy'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time \n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "    'comp.graphics',\n",
    "    'sci.space',\n",
    "]\n",
    "\n",
    "\n",
    "def size_mb(docs):\n",
    "    return sum(len(s.encode('utf-8')) for s in docs)/1e6\n",
    "\n",
    "def load_dataset(verbose = False, remove = ()):\n",
    "    \"\"\"Loading and vectorizing the 20 newsgroup dataset\"\"\"\n",
    "    \n",
    "    data_train = fetch_20newsgroups(\n",
    "        subset = 'train',\n",
    "        categories = categories, \n",
    "        shuffle = True, \n",
    "        random_state=42, \n",
    "        remove = remove\n",
    "    )\n",
    "    \n",
    "    data_test = fetch_20newsgroups(\n",
    "        subset = 'test',\n",
    "        categories = categories, \n",
    "        shuffle = True, \n",
    "        random_state=42, \n",
    "        remove = remove\n",
    "    )\n",
    "    \n",
    "    #order of labels in 'target_names' can be different from categories\n",
    "    \n",
    "    target_names = data_train.target_names\n",
    "    \n",
    "    #Splitting the target in a training set and a test set\n",
    "    \n",
    "    y_train, y_test = data_train.target, data_test.target\n",
    "    \n",
    "    #Extracting features from the training data using the sparse vectorizer\n",
    "    \n",
    "    t0 = time()\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(sublinear_tf = True, max_df = 0.5, min_df = 5, stop_words = 'english')\n",
    "    \n",
    "    X_train  = vectorizer.fit_transform(data_train.data)\n",
    "    \n",
    "    duration_train = time() - t0\n",
    "    \n",
    "    #Extracting features from the test data  using the TFIDFvectorizer\n",
    "    \n",
    "    t0 = time()\n",
    "    \n",
    "    X_test = vectorizer.fit_transform(data_test.data)\n",
    "    \n",
    "    duration_test = time() - t0\n",
    "    \n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    if verbose: \n",
    "        \n",
    "        #compute the size of loaded data\n",
    "        \n",
    "        data_train_size_mb = size_mb(data_train.data)\n",
    "        data_test_size_mb = size_mb(data_test.data)\n",
    "        \n",
    "        print(\n",
    "        f'{len(data_train.data)} documents - '\n",
    "        f'{data_train_size_mb:.2f}MB (training set)'\n",
    "         )\n",
    "    \n",
    "        print(\n",
    "        f'{len(data_test.data)} documents - '\n",
    "        f'{data_test_size_mb:.2f}MB (training set)'\n",
    "        )\n",
    "    \n",
    "        print(f'{len(target_names)} categories')\n",
    "\n",
    "        print(f'vectorize training done in {duration_train:.3f}s'\n",
    "          f'at {data_train_size_mb / duration_train:.3f} MB/s')\n",
    "    \n",
    "        print(f'n_samples: {X_train.shape[0]}, n_features: {X_test.shape[1]}')\n",
    "    \n",
    "        print(f'vectorize testing done in {duration_test:.3f}s'\n",
    "          f'at {data_test_size_mb / duration_test:.3f} MB/s')\n",
    "    return X_train, X_test, y_train, y_test, feature_names, target_names\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
